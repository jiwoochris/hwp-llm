import streamlit as st
from langchain.schema import ChatMessage

def single():
    import streamlit as st

    import olefile
    from langchain.text_splitter import CharacterTextSplitter
    from langchain.vectorstores import FAISS
    from langchain.embeddings import OpenAIEmbeddings

    from langchain.chat_models import ChatOpenAI

    from langchain.schema import HumanMessage, SystemMessage

    from langchain.callbacks.base import BaseCallbackHandler
    from langchain.schema import ChatMessage

    # from openai import OpenAI

    from dotenv import load_dotenv

    load_dotenv()

    class StreamHandler(BaseCallbackHandler):
        def __init__(self, container, initial_text=""):
            self.container = container
            self.text = initial_text

        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.text += token
            self.container.markdown(self.text)


    # Function to extract text from an HWP file
    import olefile
    import zlib
    import struct

    def get_hwp_text(filename):
        f = olefile.OleFileIO(filename)
        dirs = f.listdir()

        # HWP íŒŒì¼ ê²€ì¦
        if ["FileHeader"] not in dirs or \
        ["\x05HwpSummaryInformation"] not in dirs:
            raise Exception("Not Valid HWP.")

        # ë¬¸ì„œ í¬ë§· ì••ì¶• ì—¬ë¶€ í™•ì¸
        header = f.openstream("FileHeader")
        header_data = header.read()
        is_compressed = (header_data[36] & 1) == 1

        # Body Sections ë¶ˆëŸ¬ì˜¤ê¸°
        nums = []
        for d in dirs:
            if d[0] == "BodyText":
                nums.append(int(d[1][len("Section"):]))
        sections = ["BodyText/Section"+str(x) for x in sorted(nums)]

        # ì „ì²´ text ì¶”ì¶œ
        text = ""
        for section in sections:
            bodytext = f.openstream(section)
            data = bodytext.read()
            if is_compressed:
                unpacked_data = zlib.decompress(data, -15)
            else:
                unpacked_data = data
        
            # ê° Section ë‚´ text ì¶”ì¶œ    
            section_text = ""
            i = 0
            size = len(unpacked_data)
            while i < size:
                header = struct.unpack_from("<I", unpacked_data, i)[0]
                rec_type = header & 0x3ff
                rec_len = (header >> 20) & 0xfff

                if rec_type in [67]:
                    rec_data = unpacked_data[i+4:i+4+rec_len]
                    section_text += rec_data.decode('utf-16')
                    section_text += "\n"

                i += 4 + rec_len

            text += section_text
            text += "\n"

        return text




    def process_uploaded_file(uploaded_file):
        # Load document if file is uploaded
        if uploaded_file is not None:
            # loader

            raw_text = get_hwp_text(uploaded_file)
            print(raw_text)
        
            
            # splitter
            text_splitter = CharacterTextSplitter(
                separator = "\r\n",
                chunk_size = 1000,
                chunk_overlap  = 200,
                length_function = len,
                is_separator_regex = False,
            )
            all_splits = text_splitter.create_documents([raw_text])
            
            print(all_splits)
            print("ì´ " + str(len(all_splits)) + "ê°œì˜ passage")
            
            # storage
            vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
                    
            return vectorstore, raw_text
        return None

    def generate_response(query_text, vectorstore, callback):

        # retriever 
        docs  = vectorstore.similarity_search(query_text)
        
        # generator
        llm = ChatOpenAI(model_name="gpt-4", temperature=0, streaming=True, callbacks=[callback])
        
        if len(docs) < 3:
            rag_prompt = [
            SystemMessage(
                content="ë„ˆëŠ” í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› hwp ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ \"í‚¤ì™€\"ì•¼. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ë§ˆ."
            ),
            HumanMessage(
                content=f"ì§ˆë¬¸:{query_text}\n\në¬¸ì„œ:{docs[0].page_content}"
            ),
        ]
            
        else:
            rag_prompt = [
                SystemMessage(
                    content="ë„ˆëŠ” í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› hwp ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ \"í‚¤ì™€\"ì•¼. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ë§ˆ."
                ),
                HumanMessage(
                    content=f"ì§ˆë¬¸:{query_text}\n\në¬¸ì„œ1:{docs[0].page_content}\në¬¸ì„œ2:{docs[1].page_content}\në¬¸ì„œ3:{docs[2].page_content}"
                ),
            ]
        
        print(rag_prompt)
        
        response = llm(rag_prompt)
        
        print(response.content)

        return response.content


    def generate_summarize(raw_text, callback):


        # generator
        llm = ChatOpenAI(model_name="gpt-4-1106-preview", temperature=0, streaming=True, callbacks=[callback])
        
        rag_prompt = [
            SystemMessage(
                content="ë‹¤ìŒ ë‚˜ì˜¬ ë¬¸ì„œë¥¼ 'Notion style'ë¡œ ìš”ì•½í•´ì¤˜. ì¤‘ìš”í•œ ë‚´ìš©ë§Œ."
            ),
            HumanMessage(
                content=raw_text
            ),
        ]
        
        print(rag_prompt)
        
        response = llm(rag_prompt)
        
        print(response.content)

        return response.content

    # File upload
    uploaded_file = st.file_uploader('Upload an article', type='hwp')

    # File upload logic
    if uploaded_file:
        vectorstore, raw_text = process_uploaded_file(uploaded_file)
        if vectorstore:
            st.session_state['vectorstore'] = vectorstore
            st.session_state['raw_text'] = raw_text

    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            ChatMessage(
                role="assistant", content='ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› hwp ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ "í‚¤ì™€"ì…ë‹ˆë‹¤. ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?'
            )
        ]

    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

    if prompt := st.chat_input("'ìš”ì•½'ì´ë¼ê³  ì…ë ¥í•´ë³´ì„¸ìš”!"):
        st.session_state.messages.append(ChatMessage(role="user", content=prompt))
        st.chat_message("user").write(prompt)

        with st.chat_message("assistant"):
            stream_handler = StreamHandler(st.empty())
            
            if prompt == "ìš”ì•½":
                response = generate_summarize(st.session_state['raw_text'], stream_handler)
                st.session_state["messages"].append(
                    ChatMessage(role="assistant", content=response)
                )
            
            else:
                response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)
                st.session_state["messages"].append(
                    ChatMessage(role="assistant", content=response)
                )
            
    # streamlit run demo.py
    

def dual():
    import streamlit as st

    import olefile
    from langchain.text_splitter import CharacterTextSplitter
    from langchain.vectorstores import FAISS
    from langchain.embeddings import OpenAIEmbeddings

    from langchain.chat_models import ChatOpenAI

    from langchain.schema import HumanMessage, SystemMessage

    from langchain.callbacks.base import BaseCallbackHandler
    from langchain.schema import ChatMessage

    # from openai import OpenAI

    from dotenv import load_dotenv

    load_dotenv()

    class StreamHandler(BaseCallbackHandler):
        def __init__(self, container, initial_text=""):
            self.container = container
            self.text = initial_text

        def on_llm_new_token(self, token: str, **kwargs) -> None:
            self.text += token
            self.container.markdown(self.text)


    # Function to extract text from an HWP file
    import olefile
    import zlib
    import struct

    def get_hwp_text(filename):
        f = olefile.OleFileIO(filename)
        dirs = f.listdir()

        # HWP íŒŒì¼ ê²€ì¦
        if ["FileHeader"] not in dirs or \
        ["\x05HwpSummaryInformation"] not in dirs:
            raise Exception("Not Valid HWP.")

        # ë¬¸ì„œ í¬ë§· ì••ì¶• ì—¬ë¶€ í™•ì¸
        header = f.openstream("FileHeader")
        header_data = header.read()
        is_compressed = (header_data[36] & 1) == 1

        # Body Sections ë¶ˆëŸ¬ì˜¤ê¸°
        nums = []
        for d in dirs:
            if d[0] == "BodyText":
                nums.append(int(d[1][len("Section"):]))
        sections = ["BodyText/Section"+str(x) for x in sorted(nums)]

        # ì „ì²´ text ì¶”ì¶œ
        text = ""
        for section in sections:
            bodytext = f.openstream(section)
            data = bodytext.read()
            if is_compressed:
                unpacked_data = zlib.decompress(data, -15)
            else:
                unpacked_data = data
        
            # ê° Section ë‚´ text ì¶”ì¶œ    
            section_text = ""
            i = 0
            size = len(unpacked_data)
            while i < size:
                header = struct.unpack_from("<I", unpacked_data, i)[0]
                rec_type = header & 0x3ff
                rec_len = (header >> 20) & 0xfff

                if rec_type in [67]:
                    rec_data = unpacked_data[i+4:i+4+rec_len]
                    section_text += rec_data.decode('utf-16')
                    section_text += "\n"

                i += 4 + rec_len

            text += section_text
            text += "\n"

        return text




    def process_uploaded_file(uploaded_file):
        # Load document if file is uploaded
        if uploaded_file is not None:
            # loader

            raw_text = get_hwp_text(uploaded_file)
            print(raw_text)
        
            
            # splitter
            text_splitter = CharacterTextSplitter(
                separator = "\r\n",
                chunk_size = 1000,
                chunk_overlap  = 200,
                length_function = len,
                is_separator_regex = False,
            )
            all_splits = text_splitter.create_documents([raw_text])
            
            print(all_splits)
            print("ì´ " + str(len(all_splits)) + "ê°œì˜ passage")
            
            # storage
            vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
                    
            return vectorstore, raw_text
        return None

    def generate_response(query_text, vectorstore_1, vectorstore_2, callback):

        # retriever 1
        docs_1  = vectorstore_1.similarity_search(query_text)
        
        # retriever 2
        docs_2  = vectorstore_2.similarity_search(query_text)
        
        # generator
        llm = ChatOpenAI(model_name="gpt-4", temperature=0, streaming=True, callbacks=[callback])
        
        rag_prompt = [
            SystemMessage(
                content="ë„ˆëŠ” í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› hwp ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ \"í‚¤ì™€\"ì•¼. ì „ë…„ë„(2021ë…„) ë¬¸ì„œì™€ ì˜¬í•´(2022ë…„) ë¬¸ì„œë¥¼ ë¹„êµí•˜ê³  ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ë§ˆ."
            ),
            HumanMessage(
                content=f"ì§ˆë¬¸:{query_text}\n\nì „ë…„ë„(2021ë…„) ë¬¸ì„œ:{docs_1[0].page_content}\n\nì˜¬í•´(2022ë…„) ë¬¸ì„œ:{docs_2[0].page_content}"
            ),
        ]
        
        print(rag_prompt)
        
        response = llm(rag_prompt)
        
        print(response.content)

        return response.content


    # First file upload
    uploaded_file_1 = st.file_uploader('ì „ë…„ë„ ë¬¸ì„œë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”', type='hwp')

    # Second file upload
    uploaded_file_2 = st.file_uploader('ì˜¬í•´ ë¬¸ì„œë¥¼ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”', type='hwp')

    # File upload logic
    if uploaded_file_1:
        vectorstore, raw_text = process_uploaded_file(uploaded_file_1)
        if vectorstore:
            st.session_state['vectorstore_1'] = vectorstore
            st.session_state['raw_text_1'] = raw_text
            
    # File upload logic
    if uploaded_file_2:
        vectorstore, raw_text = process_uploaded_file(uploaded_file_2)
        if vectorstore:
            st.session_state['vectorstore_2'] = vectorstore
            st.session_state['raw_text_2'] = raw_text

    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            ChatMessage(
                role="assistant", content='ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› hwp ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ "í‚¤ì™€"ì…ë‹ˆë‹¤. ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?'
            )
        ]

    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

    if prompt := st.chat_input("ë‘ ë¬¸ì„œë¥¼ ë¹„êµí•˜ëŠ” ì§ˆë¬¸ì„ ì‘ì„±í•´ì£¼ì„¸ìš”"):
        st.session_state.messages.append(ChatMessage(role="user", content=prompt))
        st.chat_message("user").write(prompt)

        with st.chat_message("assistant"):
            stream_handler = StreamHandler(st.empty())

            response = generate_response(prompt, st.session_state['vectorstore_1'], st.session_state['vectorstore_2'], stream_handler)
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response)
            )
            
    # streamlit run demo.py 
    
def main():
    # Page title
    st.set_page_config(page_title='ğŸ¦œğŸ”— í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› ì‚¬ì—… ì„±ê³¼ ìš”ì•½ë´‡')
    st.title('ğŸ¦œğŸ”— í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› ì‚¬ì—… ì„±ê³¼ ìš”ì•½ë´‡')
    
    # í˜„ì¬ í˜ì´ì§€ ìƒíƒœ ì´ˆê¸°í™”
    if "current_page" not in st.session_state:
        st.session_state["current_page"] = None

    # Radio button for page selection
    page = st.radio("Choose a page:", ('Single Document', 'Dual Document'))
    
    # í˜ì´ì§€ê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œ ë©”ì‹œì§€ ì´ˆê¸°í™”
    if page != st.session_state["current_page"]:
        st.session_state["messages"] = [
            ChatMessage(
                role="assistant", content='ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì²­ì†Œë…„í™œë™ì§„í¥ì› hwp ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì±—ë´‡ "í‚¤ì™€"ì…ë‹ˆë‹¤. ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?'
            )
        ]
        st.session_state["current_page"] = page

    # Conditional function calls
    if page == 'Single Document':
        single()
    elif page == 'Dual Document':
        dual()

if __name__ == "__main__":
    main()